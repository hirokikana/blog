---
layout: post
title: HPL
date: 2019/10/05 12:00:00 +09:00
type: post
published: false
status: publish
categories:
- dev
tags:
- hpl
---

 - スーパーコンピュータの性能ランキングといえばTOP500
 - TOP500はHPLというベンチマークが利用されている
 - スーパーコンピュータといえば京や地球シミュレーターのような大きな施設のイメージ
 - ランキングを見ると専用ハードウェアではなく、意外と我々が普通に利用できるマシンもある。AWSインスタンスとか
 - 意外と自分のマシンでも動くじゃないかという気持ちになり、試しにHPLを動かしてみた記録
 - HPLが何をしているかというのは、理解力が追いつかなくてよくわからなかった
 - とりあえず動かすだけが目的
## はじめに


## 環境
とりあえず動けば何でもよかったので、下記の環境で検証を実施しました。
  - Host
    - MacBook Air / 1.3 GHz Intel Core i5 / 8 GB 1600 MHz DDR3
  - Guest
    - VirtualBox 6.0.12
    - CentOS 7.2 / 1 GB
HPLの実装はテネシー大学の Innovative Computing Laboratory (ICL)で実装が提供されています。これをベースにIntel MKLを利用した最適化されたバイナリが提供されてますが、試すだけならこれが一番手軽に試せると思います。今回は動かすのが目的ですし、せっかくなのでICL提供のものを利用してみることにしました。
HPLはMPI(Message Passing Interface / 1.1準拠)とBLAS(Basic Linear Algebra Subprograms)もしくはVSIPL(Vector Signal Image Processing Library)の実装が必要です。一貫性がないことを言いますが、これはソースから入れるのがめんどくさかったのでCentOSで提供されているものを利用しました。
  - HPL 2.3
  - OpenMPI 1.10.0
  - OpenBLAS 0.2.15

## 構築
CentOSの環境がある前提で進めます。
まず、MPIとBLASの実装であるOpenMPI / OpenBLASをyumでインストールします。

```
sudo yum install -y openmpi openblas-devel openblas* openmpi-devel
export PATH=$PATH:/usr/lib64/openmpi/bin
```
OpenMPIに含まれるmpicc / mpirunはなぜか/usr/lib64/openmpi/bin以下に配置されているので、PATHを通しておきます。
恒常的に設定するには.bashrc等に含めておく必要があります。

ここまででHPLをコンパイルする準備が整いました。
HPLをダウンロード・展開します。Makefileが環境ごとにいくつかあるのですが、参考サイトにあったものを利用しました。
名前からPentium IIでCBLASをを利用する際に適切な設定がされているものなのでしょうか。
```
wget https://www.netlib.org/benchmark/hpl/hpl-2.3.tar.gz
tar xzvf hpl-2.3.tar.gz
cd hpl-2.3
cp setup/Make.Linux_PII_CBLAS_gm .
```

環境に合わせてMakefileを編集する必要があります。下記がdiffで、ご自身の環境に合わせて設定してください。
```
70c70
< TOPdir       = /home/vagrant/hpl-2.3
---
> TOPdir       = $(HOME)/hpl
95c95
< LAdir        = /usr/lib64
---
> LAdir        = $(HOME)/netlib/ARCHIVES/Linux_PII
97c97
< LAlib        = $(LAdir)/libopenblas.a
---
> LAlib        = $(LAdir)/libcblas.a $(LAdir)/libatlas.a
```

最後にコンパイルするとバイナリが出力されるので、実行します。
```
make arch=Linux_PII_CBLAS_gm
cd bin/Linux_PII_CBLAS_gm/
mpirun -np 4 ./xhpl
```

## まとめ
単一のノードで実行しましたが、MPIに準拠していれば複数のノードで並列に実行することができます(検証してませんが、理屈上はできると理解)。HPLで利用されている計算がどの部分が並列化するとどのような効果があるかということについては全く理解をしていないのですが、そのあたりがHPLで最適化・パフォーマンスチューニングを行ううえでの醍醐味なのだろうと思います。

今後の目的としては、HPLの内容について基本的な理解を深めるというあたりでしょうか。

## 参考サイト
 - [HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers](https://www.netlib.org/benchmark/hpl/)
 - [電子計算記 個人的な検証を - 15. MPIクラスターを作ろう！ - HPLを動かしてみる](http://fujish.hateblo.jp/entry/2018/01/11/003736)
 - [Intel® Math Kernel Library (Intel® MKL) Benchmarks](https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite)
 - [TOP500 Supercomputer Sites](https://www.top500.org/)
