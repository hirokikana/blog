---
layout: post
title: TOP500と同じベンチマークをMacBook Air上の仮想マシンで実行する
date: 2019/10/06 15:00:00 +09:00
type: post
published: true
status: publish
categories:
- dev
tags:
- hpl
- TOP500
- Linpack
---
用途もないのにスペックの高いマシンを買って、時間のかかるミドルウェア等のコンパイルやメディアのエンコードの速度が向上しているのを見ると楽しいと思いませんか？私は楽しいです。
さて、そのような趣向がある私ですが、もう少し比較対象が多いベンチマークをやってみたくなり、TOP500に注目しました。

TOP500は、よくスーパーコンピュータの性能ランキングの話の際に出てくるアレです。「2位じゃダメなんですか」的な話のときにも話題になったアレです。
今まで勝手にいわゆる建物ごと作るようなスーパーコンピュータでないと、ベンチマークすら走らせられないのかなと勝手に思っていたのですが、よくよく調べてみると手元のマシンでも動かせることがわかったので、今回はその記録です。

## TOP500とは
[TOP500](https://www.top500.org/)とはスーパーコンピュータの性能ランキングとしてよく話題に出ていますが、実際にはHPLというベンチマークの結果のランキングであることがわかりました。
HPLはLINPACKというベンチマークを並列に実行するために新たに作られたもので、TOPS50ではそれがずっと使われているようです。
[ランキング](https://www.top500.org/lists/2019/06/)は公開されており、どの国のどのようなシステムがどのようなハードウェアで実行し、どのような結果であったかということが参照できます。

ランキングを見ると、2019年6月ランキングの[136位](https://www.top500.org/site/50808)にはAmazon EC2 C5 instanceを使ってクラスタを組んだ構成がランクインしています。
これを見るとなんとなく、自分でも現実的に用意できるのではないかという気持ちになりませんか？Amazon EC2 C5 instanceなら買おうと思えば、一応すぐ用意できそうじゃないですか(そんな大量に立ち上げるお金ないですが……
)。

## 目的
今回は、私が持ってるような誰でも買えるマシンを使って、TOP500で利用されているHPLで性能測定をするということを目的としています。
数年前の500位以内に入るようにどうこうしようとか、そのようなことは全く考えていないです。

実際に何をやっているか理解しないで値だけを見るというのは、自分のふるまいとして正しいのかという気持ちにはなりましたが、まあまず動かすところからかな……ということで気持ちを落ち着けました。

## 環境
下記の環境で検証を実施しました。

 - Host
    - MacBook Air / 1.3 GHz Intel Core i5 / 8 GB 1600 MHz DDR3
  - Guest
    - VirtualBox 6.0.12
    - CentOS 7.2 / 1 GB 

HPLは[テネシー大学の Innovative Computing Laboratory (ICL)で実装が提供](https://www.netlib.org/benchmark/hpl/)されています。これをベースにIntel MKLを利用した最適化されたバイナリが提供されており、試すだけならこれが一番手軽に試せると思います。今回は動かすのが目的ですし、せっかくなのでICL提供のものを利用してみることにしました。
HPLはMPI(Message Passing Interface / 1.1準拠)とBLAS(Basic Linear Algebra Subprograms)もしくはVSIPL(Vector Signal Image Processing Library)の実装が必要です。一貫性がないことを言いますが、これはソースから入れるのがめんどくさかったのでCentOSで提供されているものyumでインストールしました。
  - HPL 2.3
  - OpenMPI 1.10.0
  - OpenBLAS 0.2.15

## 構築
CentOSの環境がある前提で進めます。
まず、MPIとBLASの実装であるOpenMPI / OpenBLASをyumでインストールします。

```
sudo yum install -y openmpi openblas-devel openblas* openmpi-devel
export PATH=$PATH:/usr/lib64/openmpi/bin
```
OpenMPIに含まれるmpicc / mpirunを利用するのですが、なぜか/usr/lib64/openmpi/bin以下に配置されているので、PATHを通しておきます。
恒常的に設定するには.bashrc等に含めておく必要があります。

ここまででHPLをコンパイルする準備が整いました。
HPLをダウンロード・展開します。Makefileが環境ごとにいくつかあるのですが、参考サイトにあったものを利用しました。
名前からPentium IIでCBLASをを利用する際に適切な設定がされているものなのでしょうか。
```
wget https://www.netlib.org/benchmark/hpl/hpl-2.3.tar.gz
tar xzvf hpl-2.3.tar.gz
cd hpl-2.3
cp setup/Make.Linux_PII_CBLAS_gm .
```

環境に合わせてMakefileを編集する必要があります。下記がdiffで、ご自身の環境に合わせて設定してください。
```
70c70
< TOPdir       = /home/vagrant/hpl-2.3
---
> TOPdir       = $(HOME)/hpl
95c95
< LAdir        = /usr/lib64
---
> LAdir        = $(HOME)/netlib/ARCHIVES/Linux_PII
97c97
< LAlib        = $(LAdir)/libopenblas.a
---
> LAlib        = $(LAdir)/libcblas.a $(LAdir)/libatlas.a
```

最後にコンパイルするとバイナリが出力されるので、実行します。
```
make arch=Linux_PII_CBLAS_gm
cd bin/Linux_PII_CBLAS_gm/
mpirun -np 4 ./xhpl
```

下記のような出力が出るのですが、全部passedとなっているのできちんと動いています。
```
(略)
--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.53029565e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR00R2R4        1000     4     4     1               0.09             7.5169e+00
HPL_pdgesv() start time Sun Oct  6 03:55:03 2019

HPL_pdgesv() end time   Sun Oct  6 03:55:03 2019

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.02169456e-03 ...... PASSED
================================================================================

Finished    864 tests with the following results:
            864 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
```
ベンチマークで利用するパラメーターはカレントディレクトリのHPL.datで指定するのですが、これが何を示しているのかまだ理解していません。
Intel MKLのドキュメントにある[HPL Application Note](https://software.intel.com/en-us/articles/performance-tools-for-software-developers-hpl-application-note/)のTuningを雑に読んだところによると問題サイズとブロックサイズが重要な要素になっているように見受けられるので、クラスタの構成に合わせてチューニングしていくということなのでしょうか。

私が行った直近の結果を見ると7.5Gflopsとあるので、比較のために1994年11月TOP500のランキングを見たところ、[89位のコンピュータ](https://www.top500.org/site/47530)が7.4Gflopsでした。1994年のスーパーコンピューターが持ち運べていると思うと、どこか感慨深いものがありますね。

## まとめ
単一のノードで実行しましたが、複数のノードで並列に実行するところから本番のようです。実際にどのあたりが勘所なのか理解が及んでいませんが、各種パラメータやクラスタの構成あたりがHPLで最適化・パフォーマンスチューニングを行ううえでの醍醐味なのだろうと思っています。

今後の目的としては、そもそもLinkpackベンチマークが何をしているか理解するあたりでしょうか。

## 参考サイト
 - [HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers](https://www.netlib.org/benchmark/hpl/)
 - [電子計算記 個人的な検証を - 15. MPIクラスターを作ろう！ - HPLを動かしてみる](http://fujish.hateblo.jp/entry/2018/01/11/003736)
 - [Intel® Math Kernel Library (Intel® MKL) Benchmarks](https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite)
 - [TOP500 Supercomputer Sites](https://www.top500.org/)
